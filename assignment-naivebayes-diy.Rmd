---
title: "Assigment - Naive Bayes DIY"
author: 
  - name author here - Stijn Kuipers(432909)
  - name reviewer here - No reviewer. Just for practice
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```
---

Choose a suitable dataset from [this](https://github.com/HAN-M3DM-Data-Mining/assignments/tree/master/datasets) folder and train your own Naive Bayes model. Follow all the steps from the CRISP-DM model.


## Business Understanding
In 2020 spam account for more than 50% of total e-mail traffic (“Spam Statistics: Spam e-Mail Traffic Share 2019” n.d.). This illustrates the value of a good spam filter. Naive Bayes spam filtering is a standard technique for handling spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s.



## Data Understanding


```{r}
url <- "SMSSpamCollection"
rawDF <- read_delim(url, col_names = FALSE, delim = "\t") # using read_delim because of the base file used.
colnames(rawDF) <- c("type", "text") #changing the column names.
head(rawDF)


```

The dataset has 2 variables (columns) and 5572 observations (rows).

The variable type is of class character. As it indicates whether the message belongs to the category ham or spam we should convert it to a factor variable.
```{r}
rawDF$type <- rawDF$type %>% factor %>% relevel("spam")
class(rawDF$type)
```
We can also visually inspect the data by creating wordclouds for each sms type.
spam <- rawDF %>% filter(type == "spam")
ham <- rawDF %>% filter(type == "ham")

wordcloud(spam$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(ham$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))

The type of words are much different from eachother. in a way more friendly.

## Data Preparation

First we need to create a corpus, which refers to a collection of text documents. In our case each sms is considered a text document. We’ll use the Corpus() function from thetm package.


```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(rawCorpus[1:3])
```
The corpus contains 5572 documents. Which obviously matches with the number of rows in our dataset.

We will use the function tm_map() to do some first cleaning up. First we’ll change everything to lowercase. We’ll also remove numbers as these will contain litle information on a message being spam or not.

For computation efficiency it is important to eliminate all items from a dataset of which you’re rather confident that they’ do’ll add little information to your model. In our case we can expect that words like “and” or “but” will be equally common in both ham and spam messages. We should therefore filter them out before we start modeling. We’ll also remove punctuation.

Now that we have removed certain items, the text lines contain a lot of whitespaces where these items used to be. In our last step we will remove additional whitespace.

```{r}
cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)
```
Let compare the clean version wit the raw version:
```{r}
tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])
```
Now that we have cleaned up the texts, we are going to transform the messages to a matrix. Each word in the each message will get its own column, each row will be a message and the cells of the matrix will contain a word count.

```{r}
cleanDTM <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDTM)
```
Before we start modeling we need to split all datasets into train and test sets. For this we will use a function from the caret package. The function createDataPartition() can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. In this case we’ll create a 75/25% split.

```{r}
# Create split indices
set.seed(1234)
trainIndex <- createDataPartition(rawDF$type, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```
```{r}
# Apply split indices to DF
trainDF <- rawDF[trainIndex, ]
```

```{r}
testDF <- rawDF[-trainIndex, ]

# Apply split indices to Corpus
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

# Apply split indices to DTM
trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]
```

As you can check (how?) the DTM has almost 7800 features. Remember that each feature in the DTM is a word. Some words will have very few counts and therefore will have limited predictive power. To save on computation time we will eliminate words with low frequencies.

```{r}
freqWords <- trainDTM %>% findFreqTerms(5)
trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))
```

With this operation we’ve reduced the number of features to around 1200.

Another issue is that the Naive Bayes classifier is typically trained on categorical features. We now have numerical matrix with word counts. We will transform the counts into a factor that simply indicates whether the word appears in the document or not. We’ll first build our own function for this and then apply it to each column in the DTM.

```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}
```


```{r}
nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```


## Modeling

We have now everything in place to start training our model and evaluate against our test dataset. The naiveBayes() function is part of the e1071 package. It takes in the features and labels of our training dataset and returns a trained model.

```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$type, laplace = 1)
```

The model van be applied to the test features using the predict() funtion which generates a vector of predictions. Using a confusion matrix we can analyze the performance of our model.

```{r}
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$type, positive = "spam", dnn = c("Prediction", "True"))
```

## Evaluation and Deployment

###Questions:

#### What do you think is the role of the laplace parameter in the naiveBayes() function?
Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. So when a word does not exist in one of the data sets it doesn't return as zero in the calculation. by adding 1 it will only give a small probability to whether that word is found in spam or normal message.

#### How would you assess the overall performance of the model?
I would say the spam filter works pretty good. Given the limitation of naive bayes. Because naive bayes gives each word the same weight as other words. so for every 100 sms i recieve 3 spamm ones. ideal = +/- 99%

#### What would you consider as more costly: high false negatives or high false positives levels? Why?
Since the model predicts spam =/= True positive. And I can imagine you don't want your real sms in your spam. So  False Negatives are more costly. True negative are your real sms.
